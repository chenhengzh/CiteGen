{
  "Filename": "Parameter interpolation adversarial training for robust image classification",
  "PaperInfo": "Xin Liu, Yichen Yang, Kun He, and John E. Hopcroft - IEEE Transactions on Information Forensics and Security (2025) - ieeexplore.ieee.org",
  "Citations": [
    {
      "Text": "CFA [38] customizes training configurations for different classes to enhance both robustness and fairness in adversarial training, addressing disparities in robustness among classes.",
      "Analysis": "描述了被引论文的方法和贡献，但没有明确称赞或深入参考其内容。",
      "Positive": false
    },
    {
      "Text": "We compare the PIAT integrated with NMSE regularization with the following AT baselines: ALP [10], TRADES [11], MART [23], MAIL [16], CFA [38], RAT [30] on CNNs.",
      "Analysis": "作为基线方法之一进行列举，没有特别评价。",
      "Positive": false
    }
  ],
  "AnalyzedSnippetIndices": [
    1,
    2,
    3,
    4,
    5
  ],
  "EncounteredExceptions": [],
  "Snippets": [
    "ass 2\n(b) Data distribution in 3D\nFig. 2: The data distributions of the toy example, which is\ntwo concentric circles with different radii. The class 1 data\nare primarily located within the inner, while the class 2 data\nare mainly distributed on the outside.\nof both adversarial examples and neighbouring data points.\nARD and PRM [37] is the first work, which proposes using\nrandomly masking gradients from some attention blocks or\nmasking perturbations to improve the adversarial robustness of\nViTs. CFA [38] customizes training configurations for different\nclasses to enhance both robustness and fairness in adversarial\ntraining, addressing disparities in robustness among classes.\nThe works most related to ours are KDSWA [39] and\nALP [10]. KDSWA [39] introduces SWA [40], which uses\nrandom weight average to smooth model weights and miti-\ngates the overfitting issue. Instead of training one model and\nrandom ensemble on another like SWA, our PIAT framework\ninterpolates the previous and current model par",
    "setting [37] to finetune the vari-\nous ViTs. Specifically, models are pre-trained on ImageNet-1K\nand are adversarially trained for 40 epochs using SGD with\nweight decay 1 × 10−4, and an initial learning rate of 0.1 that\nis divided by 10 at the 36th and 38th epochs. Simple data\naugmentations such as random crop with padding and random\nhorizontal flips are applied.\nWe compare the PIAT integrated with NMSE regularization\nwith the following AT baselines: ALP [10], TRADES [11],\nMART [23], MAIL [16], CFA [38], RAT [30] on CNNs.\nMoreover, we also evaluate the performance of PIAT inte-\ngrated with ARD and PRM (A&P) [37] on ViTs. We adopt\nvarious adversarial attacks to evaluate the defense efficacy of\nour method, including PGD [26], CW [27] and AutoAttack\n(AA) [28].\nIEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL.XX, NO.XX, JULY 2024\n7\nTABLE I: The clean and robust accuracy (%) of our methods (PIAT+NMSE) and defense baselines using ResNet18 model\ntrained on CIFAR10, CIFAR100 and SVHN datas",
    "in each column is in bold.\nDataset\nMethod\nClean\nPGD20\nCW\nAA\nBest\nFinal\nDiff\nBest\nFinal\nDiff\nBest\nFinal\nDiff\nBest\nFinal\nDiff\nCIFAR10\nPGD-AT\n84.28\n85.62\n1.34\n50.29\n45.86\n4.43\n49.31\n43.25\n6.06\n46.33\n41.36\n4.97\nALP\n79.74\n81.45\n1.71\n52.37\n48.62\n3.75\n49.60\n43.87\n5.73\n46.13\n41.88\n4.25\nTRADES\n82.39\n83.04\n0.65\n53.60\n50.74\n2.86\n50.90\n49.04\n1.86\n48.04\n46.80\n1.24\nMART\n81.91\n83.99\n2.08\n53.70\n48.63\n5.07\n49.35\n44.92\n4.43\n47.45\n43.65\n3.80\nMAIL\n82.65\n85.17\n2.49\n51.15\n47.14\n4.01\n48.88\n44.38\n4.50\n45.16\n43.02\n2.14\nCFA\n82.80\n83.88\n1.08\n53.24\n51.69\n1.55\n51.45\n49.97\n1.48\n48.40\n47.74\n0.64\nRAT\n81.63\n82.61\n0.98\n52.25\n50.09\n2.16\n49.47\n47.93\n1.54\n45.20\n44.30\n0.90\nPIAT +NMSE\n80.96\n82.84\n1.88\n53.74\n52.81\n0.93\n51.72\n50.49\n1.23\n48.80\n47.97\n0.83\nCIFAR100\nPGD-AT\n58.48\n58.53\n0.05\n28.36\n21.72\n6.64\n27.06\n21.12\n5.94\n23.85\n19.55\n4.30\nALP\n57.29\n58.65\n1.36\n28.12\n24.66\n3.46\n26.84\n22.17\n4.67\n23.57\n20.49\n3.08\nTRADES\n56.71\n56.32\n0.39\n29.19\n27.70\n1.49\n26.05\n24.53\n1.52\n23.91\n22.70\n1.21\nMART\n55.26\n57.77\n2.51\n30.10\n25.96\n4.14\n26.30\n23.79\n2.51\n24.13\n22.35\n1.78\nMAIL\n58.73\n59.00\n0.27\n27.99\n24.69\n3.30\n26.28\n23.37\n2.91\n22.50\n20.86\n1.64\nCFA\n56.28\n61.62\n5.34\n30.64\n29.74\n0.90\n27.74\n25.95\n1.79\n24.26\n21.58\n2.68\nRAT\n53.35\n56.35\n3.00\n28.69\n27.67\n1.02\n25.52\n23.69\n1.83\n23.10\n22.40\n0.70\nPIAT +NMSE\n56.04\n57.16\n1.12\n31.45\n30.87\n0.58\n28.74\n27.76\n1.02\n26.09\n25.13\n0.96\nSVHN\nPGD-AT\n93.85\n94.33\n0.48\n59.01\n52.35\n6.66\n48.66\n44.13\n4.53\n43.02\n38.66\n4.36\nALP\n92.54\n93.67\n1.13\n59.13\n55.12\n5.01\n52.22\n48.53\n3.69\n45.67\n42.41\n3.26\nTRADES\n90.88\n91.34\n0.46\n59.50\n57.04\n2.46\n52.76\n50.42\n2.34\n46.59\n44.87\n1.72\nMART\n90.84\n92.95\n2.11\n57.70\n54.29\n3.41\n52.95\n50.09\n2.86\n46.98\n43.75\n3.23\nMAIL\n90.15\n93.69\n3.54\n57.47\n54.60\n3.14\n52.78\n49.73\n3.05\n46.26\n41.24\n5.02\nCFA\n92.23\n93.68\n1.45\n60.77\n58.85\n1.92\n55.17\n52.71\n2.46\n49.64\n46.53\n3.11\nRAT\n89.23\n90.97\n1.74\n45.83\n38.25\n7.58\n44.15\n32.71\n11.44\n46.10\n22.40\n23.70\nPIAT +NMSE\n91.70\n93.07\n1.37\n61.21\n59.84\n1.37\n55.88\n54.45\n1.43\n51.29\n49.82\n1.47\nB. Evaluation on Defense Efficacy\nWe compare the defense efficacy of our method with four\nAT baselines including PGD-AT, TRADES, MART, MAIL.\nTable I reports the best and final clean and robust accuracy of\nthe ResNet18 model trained using our method or the defense\nbaselines unde",
    "amework combined with various adversarial training baseline methods under\nthe AA attack on CIFAR10, CIFAR100, and SVHN datasets using ResNet18 model.\nTABLE II: The clean and robust accuracy (%) of our meth-\nods (PIAT+NMSE) and defense baselines using WRN-32-10\nmodel on CIFAR10 and CIFAR100 datasets. The best result\nin each column is in bold.\nDataset\nMethod\nClean\nPGD20\nAA\nCIFAR10\nPGD-AT\n86.87\n48.77\n47.78\nALP\n84.18\n53.55\n49.68\nTRADES\n82.13\n55.14\n50.38\nMART\n81.57\n56.44\n49.58\nMAIL\n84.96\n52.58\n47.26\nCFA\n86.44\n57.84\n52.96\nRAT\n83.46\n57.07\n51.56\nPIAT +NMSE\n85.04\n58.04\n53.83\nCIFAR100\nPGD-AT\n59.30\n28.13\n23.99\nALP\n58.11\n28.59\n24.45\nTRADES\n57.99\n31.97\n26.76\nMART\n55.19\n31.16\n26.46\nMAIL\n58.04\n29.50\n23.97\nCFA\n63.37\n33.89\n28.98\nRAT\n60.89\n33.39\n27.95\nPIAT +NMSE\n61.04\n35.15\n30.07\nand PIAT significantly enhances the robustness of ViTs, gain-\ning an improvement of 2.85%, 2.24%, and 2.78%, respectively,\nagainst AA attacks for ConViT-B. Our framework leads to\nhigher robust accuracy when combined with other adversarial\ntraining methods on both CNNs and ViTs, indicating that PIAT\nhas good flexibility and generalization.\nTo evaluate the effectiveness of our PIAT framework on\ndifferent datasets, we also compar",
    "rsaries for improving\nadversarial training,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 24 678–24 687.\n[36] Q. Li, Y. Guo, W. Zuo, and H. Chen, “Squeeze training for adversarial\nrobustness,” in International Conference on Learning Representations,\n2023.\n[37] Y. Mo, D. Wu, Y. Wang, Y. Guo, and Y. Wang, “When adversarial train-\ning meets vision transformers: Recipes from training to architecture,” in\nNeural Information Processing Systems, 2022.\n[38] Z. Wei, Y. Wang, Y. Guo, and Y. Wang, “CFA: class-wise calibrated fair\nadversarial training,” in CVPR, 2023, pp. 8193–8201.\n[39] T. Chen, Z. Zhang, S. Liu, S. Chang, and Z. Wang, “Robust overfitting\nmay be mitigated by properly learned smoothening,” in ICLR, 2021.\n[40] P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson,\n“Averaging weights leads to wider optima and better generalization,” in\nUAI, A. Globerson and R. Silva, Eds., 2018, pp. 876–885.\n[41] H. Wang and Y. Wang, “Generalist: Decoupling natural and robust g"
  ]
}